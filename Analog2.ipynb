{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Analog.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urieliram/statistical/blob/main/Analog2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paquetes"
      ],
      "metadata": {
        "id": "JvC9Neb7LtTu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JboE4Y4YndQq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f1c8395-2772-4a44-ff55-8ad8128f734f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: dtw-python in /usr/local/lib/python3.7/dist-packages (1.1.12)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.7/dist-packages (from dtw-python) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.7/dist-packages (from dtw-python) (1.4.1)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3021, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 180, in _main\n",
            "    status = self.run(options, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/req_command.py\", line 199, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 385, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 515, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/check.py\", line 103, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/check.py\", line 45, in create_package_set_from_installed\n",
            "    package_set[name] = PackageDetails(dist.version, dist.requires())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2736, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3023, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3033, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3094, in parse_requirements\n",
            "    yield Requirement(line)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3101, in __init__\n",
            "    super(Requirement, self).__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/packaging/requirements.py\", line 113, in __init__\n",
            "    req = REQUIREMENT.parseString(requirement_string)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1943, in parseString\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4069, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4254, in parseImpl\n",
            "    ret = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4069, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4849, in parseImpl\n",
            "    loc, tokens = self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4069, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4069, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4462, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4052, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4254, in parseImpl\n",
            "    ret = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4462, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4069, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4254, in parseImpl\n",
            "    ret = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1691, in _parseNoCache\n",
            "    retTokens = ParseResults(tokens, self.resultsName, asList=self.saveAsList, modal=self.modalResults)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 570, in __init__\n",
            "    self.__tokdict = dict()\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/main.py\", line 71, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 104, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 180, in _main\n",
            "    status = self.run(options, args)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "pip install dtw-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install dtreeviz"
      ],
      "metadata": {
        "id": "BNS7utNCMGwZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8793274e-5ce4-49c4-a037-a5b33bcb6b6c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: dtreeviz in /usr/local/lib/python3.7/dist-packages (1.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from dtreeviz) (1.3.5)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from dtreeviz) (3.6.4)\n",
            "Requirement already satisfied: graphviz>=0.9 in /usr/local/lib/python3.7/dist-packages (from dtreeviz) (0.10.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from dtreeviz) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from dtreeviz) (1.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from dtreeviz) (1.21.6)\n",
            "Requirement already satisfied: colour in /usr/local/lib/python3.7/dist-packages (from dtreeviz) (0.1.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->dtreeviz) (1.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->dtreeviz) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->dtreeviz) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->dtreeviz) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->dtreeviz) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->dtreeviz) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->dtreeviz) (2022.1)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->dtreeviz) (0.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->dtreeviz) (57.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->dtreeviz) (21.4.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->dtreeviz) (1.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->dtreeviz) (1.11.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->dtreeviz) (8.13.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->dtreeviz) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->dtreeviz) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->dtreeviz) (1.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install statsmodels==0.13.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxgSsq_A0f4b",
        "outputId": "d448711b-9ced-4576-c7aa-09ba9810f3c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: statsmodels==0.13.1 in /usr/local/lib/python3.7/dist-packages (0.13.1)\n",
            "Requirement already satisfied: scipy>=1.3 in /usr/local/lib/python3.7/dist-packages (from statsmodels==0.13.1) (1.4.1)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from statsmodels==0.13.1) (0.5.2)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.7/dist-packages (from statsmodels==0.13.1) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from statsmodels==0.13.1) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->statsmodels==0.13.1) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25->statsmodels==0.13.1) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.5.2->statsmodels==0.13.1) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import timeit\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "import random\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
        "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n",
        "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV, LarsCV, Lasso, Ridge, BayesianRidge, LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error,mean_squared_error,mean_absolute_percentage_error\n",
        "from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor, AdaBoostRegressor, BaggingRegressor\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from math import sqrt\n",
        "from dtw import *\n",
        "from scipy import stats\n",
        "import statsmodels.tools.eval_measures as bias\n",
        "from statsmodels.tsa.ar_model import AutoReg\n",
        "%matplotlib inline\n",
        "from sklearn import preprocessing\n",
        "\n",
        "sns.set_theme(style=\"white\")"
      ],
      "metadata": {
        "id": "DuC_OkW4nm78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carga de datos"
      ],
      "metadata": {
        "id": "YOyUlBvpLran"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://drive.google.com/drive/folders/1Gj3XK9kM-lE18uBMe3qrZOGEm8yAI8i9\n",
        "#https://www.codegrepper.com/code-examples/python/how+to+read+csv+file+from+google+drive+on+google+colab+\n",
        "path        = 'https://drive.google.com/uc?export=download&id=' \n",
        "URL_Demanda = 'https://drive.google.com/file/d/1xcpXDTE7H6EBMLOkic5lq-lzSwiLG2ZS/view?usp=sharing'\n",
        "\n",
        "df_Demanda  = pd.read_csv(path + URL_Demanda .split('/')[-2], usecols=[0] ) #names=['CLVUNI','TYPE','NODE'], usecols=[1,2,3,4,5,6,], 1,2,3,4,5,6,7,8,9,10,11,12,13,15,\n",
        "df_Demanda.dropna(inplace=True)\n",
        "# serie = df_Demanda.to_numpy()\n",
        "# serie = StandardScaler().fit_transform(serie)\n",
        "# serie = serie.ravel()  ## Con esto quitamos el bracket o corchete en cada uno de los elementos del arreglo"
      ],
      "metadata": {
        "id": "7RW90yl-nu87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funciones"
      ],
      "metadata": {
        "id": "DgJdgEsvHkNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trunc(values, decs=0):\n",
        "    return np.trunc(values*10**decs)/(10**decs)"
      ],
      "metadata": {
        "id": "EcNqmPCJP_yS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def diff(list1,list2):\n",
        "    difference = []\n",
        "    zip_object = zip(list1, list2)\n",
        "    for list1_i, list2_i in zip_object:\n",
        "        difference.append(list1_i-list2_i)\n",
        "    return difference"
      ],
      "metadata": {
        "id": "vALZjLAUfkzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CYAN = '#76ced6' ; LILA = '#777bd4'; VERDE='#17cb49'; NARA='#ff8000'; AZUL='#168fff'; OTROAZUL = \"b-\"; ROJO= \"r-\"; MAGE=\"FF00FF\";\n",
        "def print_serie2(serie_,prototipo_,title_,ytitle_,xtitle_,sizex_=8,sizey_=5,namefile_='fig_t16_serie.png'):\n",
        "    fig, ax1 = plt.subplots(figsize=(sizex_,sizey_))\n",
        "    plt.title(title_,fontsize='x-large',color=NARA)\n",
        "    ax1.set_xlabel(xtitle_, color=NARA, fontsize='large')\n",
        "    ax1.set_ylabel(ytitle_, color=NARA, fontsize='large')\n",
        "    plt.tick_params(colors = NARA, which='both')\n",
        "    ax1.spines['bottom'].set_color(NARA)\n",
        "    ax1.spines['top'   ].set_color(NARA) \n",
        "    ax1.spines['right' ].set_color(NARA)\n",
        "    ax1.spines['left'  ].set_color(NARA)\n",
        "    if len(prototipo_) != 0: \n",
        "        plt.plot(prototipo_,alpha=0.6, linestyle='dashed', color='red', linewidth=3)\n",
        "    for p in serie_:\n",
        "        plt.plot(p,alpha=0.3, linewidth=2)    \n",
        "    plt.savefig(namefile_, transparent=True)         \n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "9cnF4RJ6no8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Preparamos la información para graficar la importancia y las posiciones en Random Forest Regressor\n",
        "def print_importances(model_,labels_,namefile_):\n",
        "    i=0\n",
        "    labels = [str(x) for x in labels_]\n",
        "    labels_importances = []\n",
        "    for feature in model_.feature_importances_:\n",
        "        labels_importances.append((feature,labels[i]))\n",
        "        i=i+1   \n",
        "    labels_importances.sort(key=lambda tup: tup[0], reverse=False)\n",
        "    importances = []\n",
        "    labels      = []\n",
        "    for tup in labels_importances:\n",
        "        importances.append(tup[0])\n",
        "        labels.append(tup[1])\n",
        "    fig, ax = plt.subplots()\n",
        "    y_pos = np.arange(len(importances))\n",
        "    ax.set_yticks(ticks=y_pos)\n",
        "    ax.barh(labels, importances, align='center',color=CYAN)\n",
        "    ax.invert_yaxis()\n",
        "    ax.set_xlabel('Importancia', color=NARA, fontsize='large')\n",
        "    ax.set_ylabel('Variable', color=NARA, fontsize='large')\n",
        "    ax.set_title( 'Importancia de las variables del bosque aleatorio', color=NARA, fontsize='large')    \n",
        "    plt.tick_params(colors = NARA, which='both')\n",
        "    ax.spines['bottom'].set_color(NARA)\n",
        "    ax.spines['top'   ].set_color(NARA) \n",
        "    ax.spines['right' ].set_color(NARA)\n",
        "    ax.spines['left'  ].set_color(NARA)\n",
        "    plt.savefig(namefile_, transparent=True)   \n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Ofhg0JCdoR-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def OLSstep(X, Y, X_2, pi_step_=0.001,verbose_=False):\n",
        "    model   = sm.OLS(Y, X)\n",
        "    results = model.fit()\n",
        "    prediction_Y2 = results.predict(X_2)\n",
        "\n",
        "    ## Ordenamos los valores 'pi' y se selecciona el más grande.\n",
        "    i = 0\n",
        "    pvalues = []\n",
        "    for pi in results.pvalues:\n",
        "        pvalues.append((i,pi))\n",
        "        i = i + 1\n",
        "    pvalues.sort(key=lambda tup: tup[1], reverse=True) ## Ordenamos por 'pi'\n",
        "    (i, pi) = pvalues[0]  \n",
        "\n",
        "    while pi > pi:\n",
        "        X   = sm.add_constant(X)\n",
        "        X_2 = sm.add_constant(X_2)   \n",
        "        if verbose_==True:\n",
        "            print('Retiramos regresor ---> X' + str(i))\n",
        "        X   = np.delete(arr=X,   obj=i+0, axis=1)\n",
        "        X_2 = np.delete(arr=X_2, obj=i+0, axis=1)   \n",
        "        model   = sm.OLS(Y, X)\n",
        "        results = model.fit()\n",
        "\n",
        "        ## Ordenamos los valores 'pi' y se selecciona el más grande\n",
        "        i = 0\n",
        "        pvalues = []\n",
        "        for pi in results.pvalues:\n",
        "            pvalues.append((i,pi))\n",
        "            i = i + 1\n",
        "        pvalues.sort(key=lambda tup: tup[1], reverse=True) ## Ordenamos por 'pi'\n",
        "        (i, pi) = pvalues[0]\n",
        "        #prediction   = results.predict(X)   ## Ajuste\n",
        "        prediction_Y2 = results.predict(X_2) ## Pronóstico\n",
        "    if len(prediction_Y2) == 0:      \n",
        "        if verbose_==True:\n",
        "            print('>>> Warning, no variable was significant in the regression.')\n",
        "        model   = sm.OLS(Y, X)\n",
        "        results = model.fit()\n",
        "        prediction_Y2 = results.predict(X_2)\n",
        "        \n",
        "    if verbose_==True:\n",
        "        print(results.summary())\n",
        "    return prediction_Y2   "
      ],
      "metadata": {
        "id": "Ls5XHfg-e5Gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RF(X, Y, X_2,labels_,typedist_,verbose_=False):\n",
        "    model         = RandomForestRegressor(random_state=42)\n",
        "    results       = model.fit(X, Y)\n",
        "    prediction_Y2 = results.predict(X_2)\n",
        "    if verbose_==True:\n",
        "        print_importances(model_=model,labels_=labels_,namefile_='fig_t16_importance_'+typedist_+'_'+'RF')\n",
        "        print('Parámetros utilizados RF')\n",
        "        print(\"bootstrap:         {}\".format(model.bootstrap))\n",
        "        print(\"n_estimators:      {}\".format(model.n_estimators))\n",
        "        print(\"max_features:      {}\".format(model.max_features))\n",
        "        print(\"max_depth:         {}\".format(model.max_depth))\n",
        "        print(\"min_samples_leaf:  {}\".format(model.min_samples_leaf))\n",
        "        print(\"min_samples_split: {}\".format(model.min_samples_split))\n",
        "    return prediction_Y2   "
      ],
      "metadata": {
        "id": "V7yMHMCWkvqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def AutoRF(X, Y, X_2,labels_,typedist_,verbose_=False):\n",
        "## https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
        "    #nestlist = []\n",
        "    #for i in range(10, 320, 50):\n",
        "    #    nestlist.append(i)\n",
        "    #param_grid = { \n",
        "    #'bootstrap': [True, False],\n",
        "    #'n_estimators': nestlist,\n",
        "    #'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    #'max_depth' : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
        "    #'min_samples_leaf': [1, 2, 4],\n",
        "    #'min_samples_split': [2, 5, 10],}\n",
        "    nestlist = []\n",
        "    for i in range(10, 320, 50):\n",
        "        nestlist.append(i)\n",
        "    param_grid = { \n",
        "        'bootstrap': [True, False],\n",
        "        'n_estimators': nestlist,\n",
        "        'max_features': ['auto', 'sqrt',],\n",
        "        'max_depth' : [10, 20, None]}\n",
        "\n",
        "    model = RandomForestRegressor(random_state=42)\n",
        "    clf   = GridSearchCV(estimator=model, param_grid=param_grid, cv=5).fit(X, Y)\n",
        "    model         = clf.best_estimator_\n",
        "    results       = model.fit(X, Y)\n",
        "    prediction_Y2 = results.predict(X_2)\n",
        "\n",
        "    if verbose_==True:\n",
        "        print_importances(model_=model,labels_=labels_,namefile_='fig_t16_importance_'+typedist_+'_'+'AutoRF')\n",
        "        print('Parameters used')\n",
        "        print(\"bootstrap:         {}\".format(model.bootstrap))\n",
        "        print(\"n_estimators:      {}\".format(model.n_estimators))\n",
        "        print(\"max_features:      {}\".format(model.max_features))\n",
        "        print(\"max_depth:         {}\".format(model.max_depth))\n",
        "        print(\"min_samples_leaf:  {}\".format(model.min_samples_leaf))\n",
        "        print(\"min_samples_split: {}\".format(model.min_samples_split))        \n",
        "    return prediction_Y2   "
      ],
      "metadata": {
        "id": "Sh3JVQq6ln4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Boosting(X, Y, X_2,typedist_,verbose_=False):\n",
        "    model         = GradientBoostingRegressor(random_state=42,)\n",
        "    results       = model.fit(X, Y)\n",
        "    prediction_Y2 = results.predict(X_2)\n",
        "    return prediction_Y2 "
      ],
      "metadata": {
        "id": "D8tPs-3mU4dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Bagging(X, Y, X_2,typedist_,verbose_=False):\n",
        "    model         = BaggingRegressor(random_state=42,)\n",
        "    results       = model.fit(X, Y)\n",
        "    prediction_Y2 = results.predict(X_2)\n",
        "    return prediction_Y2   "
      ],
      "metadata": {
        "id": "n49EThjiXHCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def AdaBoost(X, Y, X_2,typedist_,verbose_=False):\n",
        "    model         = AdaBoostRegressor(random_state=42,)\n",
        "    results       = model.fit(X, Y)\n",
        "    prediction_Y2 = results.predict(X_2)\n",
        "    return prediction_Y2 "
      ],
      "metadata": {
        "id": "XxzvQPysXiIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LinearReg(X, Y, X_2,typedist_,verbose_=False):\n",
        "    model         = LinearRegression()\n",
        "    results       = model.fit(X, Y)\n",
        "    prediction_Y2 = results.predict(X_2)\n",
        "    return prediction_Y2   "
      ],
      "metadata": {
        "id": "eWHrx7dVXYra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def BayesRidge(X, Y, X_2,typedist_,verbose_=False):\n",
        "    model         = BayesianRidge(compute_score=True)\n",
        "    results       = model.fit(X, Y)\n",
        "    prediction_Y2 = results.predict(X_2) \n",
        "    return prediction_Y2   "
      ],
      "metadata": {
        "id": "eBWFLGGLVLXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LassoReg(X, Y, X_2,typedist_,verbose_=True):\n",
        "    model         = Lasso(alpha=0.1)\n",
        "    results       = model.fit(X, Y)\n",
        "    prediction_Y2 = results.predict(X_2)\n",
        "    return prediction_Y2  "
      ],
      "metadata": {
        "id": "lNUZVyZvXvVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RidgeReg(X, Y, X_2,typedist_,verbose_=False):\n",
        "    model         = Ridge(alpha=0.1)\n",
        "    results       = model.fit(X, Y)\n",
        "    prediction_Y2 = results.predict(X_2)\n",
        "    return prediction_Y2   "
      ],
      "metadata": {
        "id": "H2ahc9j2VXLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def PLS(X, Y, X_2,n_components,typedist_,verbose_=False):\n",
        "    model         = PLSRegression(n_components=n_components)\n",
        "    results       = model.fit(X, Y)\n",
        "    prediction_Y2 = results.predict(X_2)\n",
        "    return prediction_Y2"
      ],
      "metadata": {
        "id": "BpVkXFJXxViD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def PCR(X, Y, X_2,n_components,typedist_,verbose_=False):\n",
        "## https://scikit-learn.org/stable/auto_examples/cross_decomposition/plot_pcr_vs_pls.html\n",
        "    model         = make_pipeline(PCA(n_components=n_components), LinearRegression())\n",
        "    results       = model.fit(X, Y)\n",
        "    prediction_Y2 = results.predict(X_2)\n",
        "    return prediction_Y2   "
      ],
      "metadata": {
        "id": "PrmmGibK48oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def VotingEnsemble(X, Y, X_2,verbose_=False):  \n",
        "    ## https://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_regressor.html#sphx-glr-auto-examples-ensemble-plot-voting-regressor-py\n",
        "    gb  = GradientBoostingRegressor(random_state=42)\n",
        "    rf  = RandomForestRegressor(random_state=42)\n",
        "    br  = BaggingRegressor(random_state=42)\n",
        "    ab  = AdaBoostRegressor(random_state=42)\n",
        "    gb.fit(X, Y)\n",
        "    rf.fit(X, Y)\n",
        "    br.fit(X, Y)\n",
        "    ab.fit(X, Y)\n",
        "    voting = VotingRegressor([(\"gb\",gb), (\"rf\",rf), (\"br\",br), (\"ab\",ab)]) #\n",
        "    voting.fit(X, Y)\n",
        "    prediction_Y2 = voting.predict(X_2)    \n",
        "    if verbose_ == True:\n",
        "        predgb  = gb.predict(X_2)\n",
        "        predrf  = rf.predict(X_2)\n",
        "        predbr  = br.predict(X_2)  \n",
        "        predab  = ab.predict(X_2) \n",
        "        ## https://matplotlib.org/3.5.0/api/_as_gen/matplotlib.pyplot.plot.html Markers, Line styles ,Colors\n",
        "        fig, ax = plt.subplots(figsize=(19,10))\n",
        "        plt.plot(predgb,  \"md\" , label=\"GradientBoostingRegressor\") ## go- rs \n",
        "        plt.plot(predrf,  \"b^\" , label=\"RandomForestRegressor\")\n",
        "        plt.plot(predbr,  \"go-\", label=\"BaggingRegressor\")\n",
        "        plt.plot(predab,  \"kD\" , label=\"AdaBoostRegressor\")\n",
        "        plt.plot(prediction_Y2, \"r*\", ms=10, label=\"VotingEnsemble\")\n",
        "        plt.tick_params(axis=\"x\", which=\"both\", bottom=False, top=False, labelbottom=False)\n",
        "        plt.ylabel(\"demanda estandarizada\", color=NARA, fontsize='large')\n",
        "        plt.xlabel(\"training samples\", color=NARA, fontsize='large')\n",
        "        plt.legend(loc=\"best\")\n",
        "        plt.title(\"Pronósticos de ensambles y su promedio\", color=NARA, fontsize='large')\n",
        "        plt.tick_params(colors = NARA, which='both')\n",
        "        ax.spines['bottom'].set_color(NARA)\n",
        "        ax.spines['top'   ].set_color(NARA) \n",
        "        ax.spines['right' ].set_color(NARA)\n",
        "        ax.spines['left'  ].set_color(NARA)\n",
        "        plt.savefig(\"fig_t16_VotingEnsemble\", transparent=True) \n",
        "        plt.show()\n",
        "    return prediction_Y2   "
      ],
      "metadata": {
        "id": "UFQb6V2fYha7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def VotingLinear(X, Y, X_2,verbose_=False):  \n",
        "    ## https://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_regressor.html#sphx-glr-auto-examples-ensemble-plot-voting-regressor-py\n",
        "    pl = PLSRegression(n_components=1)\n",
        "    lr = LinearRegression()\n",
        "    ri = Ridge(alpha=0.1)\n",
        "    la = Lasso(alpha=0.1)    \n",
        "    pc = make_pipeline(PCA(n_components=1), LinearRegression())\n",
        "    pl.fit(X, Y)\n",
        "    lr.fit(X, Y)\n",
        "    ri.fit(X, Y)\n",
        "    la.fit(X, Y)\n",
        "    pc.fit(X, Y)\n",
        "    voting = VotingRegressor([(\"lr\",lr),(\"ri\",ri),(\"la\",la),(\"pc\",pc)]) #,(\"pl\",pl)\n",
        "    voting.fit(X, Y)\n",
        "    prediction_Y2 = voting.predict(X_2)    \n",
        "    if verbose_ == True:\n",
        "        predpl = pl.predict(X_2)\n",
        "        predlr = lr.predict(X_2)\n",
        "        predri = ri.predict(X_2)\n",
        "        predla = la.predict(X_2)\n",
        "        predpc = pc.predict(X_2)\n",
        "        ## https://matplotlib.org/3.5.0/api/_as_gen/matplotlib.pyplot.plot.html Markers, Line styles ,Colors\n",
        "        fig, ax = plt.subplots(figsize=(17,9))\n",
        "        plt.plot(predpl,  \"md\" , label=\"PLSRegression\")\n",
        "        plt.plot(predlr,  \"b^\" , label=\"LinearRegression\")\n",
        "        plt.plot(predri,  \"go-\", label=\"Ridge\")\n",
        "        plt.plot(predla,  \"ys\" , label=\"Lasso\")\n",
        "        plt.plot(predpc,  \"cs\" , label=\"PCR\")\n",
        "        plt.plot(prediction_Y2, \"r*\", ms=10, label=\"VotingLinear\")\n",
        "        plt.tick_params(axis=\"x\", which=\"both\", bottom=False, top=False, labelbottom=False)\n",
        "        plt.ylabel(\"demanda estandarizada\", color=NARA, fontsize='large')\n",
        "        plt.xlabel(\"training samples\", color=NARA, fontsize='large')\n",
        "        plt.legend(loc=\"best\")\n",
        "        plt.title(\"Pronósticos lineales y su promedio\", color=NARA, fontsize='large')\n",
        "        plt.tick_params(colors = NARA, which='both')\n",
        "        ax.spines['bottom'].set_color(NARA)\n",
        "        ax.spines['top'   ].set_color(NARA) \n",
        "        ax.spines['right' ].set_color(NARA)\n",
        "        ax.spines['left'  ].set_color(NARA)\n",
        "        plt.savefig(\"fig_t16_VotingLinear\", transparent=True) \n",
        "        plt.show()\n",
        "    return prediction_Y2   "
      ],
      "metadata": {
        "id": "EDO3mPUv3wf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Calcula distancia euclidiana\n",
        "def euclidean(neig1, neig2):\n",
        "\t  distance = 0.0\n",
        "\t  for i in range(len(neig1)):\n",
        "\t\t    distance += (neig1[i] - neig2[i])**2\n",
        "\t  return sqrt(distance)"
      ],
      "metadata": {
        "id": "yY6yd0WJBr7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analogo_knn(serie,vsele,k=10,tol=0.8,n_components=3,typedist='pearson',typereg='OLSstep',verbose=False):    \n",
        "#    vsele      : Tamanio de la ventana de selección\n",
        "#    k          : Número de vecinos a buscar k\n",
        "#    tol        : Tolerancia de tamaño de ventanas para seleccion de vecinos\n",
        "#    typedist   : medida de distancia, 'euclidian' o 'pearson' o 'dtw' \n",
        "\n",
        "    t_o = time.time()\n",
        "    n = len(serie) ## longitud total de la serie\n",
        "\n",
        "    ## PASO 1: Selección de las ventanas de mayor correlación.\n",
        "\n",
        "    ## Calculamos la distancia entre todos los vecinos.\n",
        "    distances = []\n",
        "    Y = serie[n-vsele:n]           ## últimos datos\n",
        "    for i in range(n-2*vsele):                              \n",
        "        if  typedist == 'dtw':     ## dynamic time warping\n",
        "            dist = dtw(Y, serie[i:i+vsele]).distance  \n",
        "        elif typedist == 'euclidian':\n",
        "            dist = euclidean(Y,serie[i:i+vsele])\n",
        "        else:\n",
        "            dist = np.corrcoef(Y,serie[i:i+vsele])[1,0]\n",
        "        if dist > 0:\n",
        "            distances.append((i, dist))\n",
        "        \n",
        "    ## Calculamos el vecindario por distancia de menor a mayor y se guardan las posiciones.\n",
        "    if typedist == 'pearson':\n",
        "        ## En caso de pearson se ordena al revés, nos interesan los mayor correlación.\n",
        "        distances.sort(key=lambda tup: tup[1], reverse=True)\n",
        "    else:\n",
        "        ## En caso de pearson se ordena al revés, nos interesan los de menor distancia.\n",
        "        distances.sort(key=lambda tup: tup[1], reverse=False)\n",
        "\n",
        "    neighbors  = []\n",
        "    neighbors2 = []\n",
        "    positions  = []\n",
        "\n",
        "    ## Calculamos los k vecinos mas cercanos y guardamos las posiciones.\n",
        "    i = 0\n",
        "    for pos, dis in distances:\n",
        "        if i==0:      \n",
        "            positions.append(pos)   \n",
        "            neighbors.append(serie[pos:pos+vsele])\n",
        "            neighbors2.append(serie[pos+vsele:pos+2*vsele])  \n",
        "        else:\n",
        "            bandera = True\n",
        "            for p in positions:\n",
        "                 ## si ya teniamos una posición en la lista que pase la tolerancia, ya no la guardamos \n",
        "                if (abs(pos - p) < tol*vsele):\n",
        "                    bandera = False\n",
        "                    i = i - 1\n",
        "                    break\n",
        "            if bandera == True:\n",
        "                ## Guarda nuevo vecino\n",
        "                positions.append(pos)   \n",
        "                neighbors.append(serie[pos:pos+vsele])\n",
        "                neighbors2.append(serie[pos+vsele:pos+2*vsele])  \n",
        "                bandera = False\n",
        "        i = i + 1\n",
        "        if i == k:\n",
        "            break\n",
        "    if verbose==True:\n",
        "        print('positions KNN:', positions) ## posición de los k vecinos mas cercanos\n",
        "\n",
        "    neighbors  = np.array(neighbors)  \n",
        "    neighbors2 = np.array(neighbors2)    \n",
        "    vacia = []\n",
        "    if verbose==True:\n",
        "        print_serie2(neighbors,Y,'Selección con KNN:'+typedist,'demanda','time',8,5,'fig_t16_X_'+typedist+'_'+typereg)\n",
        "\n",
        "    t_sel = time.time() - t_o\n",
        "\n",
        "    ## PASO 2: Regresión entre los vecinos mas cercanos 'X' y la última ventana 'Y'\n",
        "\n",
        "    ## Definimos nuestros regresores   \n",
        "    X   = (neighbors.T ).tolist()\n",
        "    X_2 = (neighbors2.T).tolist()\n",
        "    Y   = (Y).tolist()\n",
        "    prediction_Y2 = []\n",
        "\n",
        "    ## -- Random forest regression --\n",
        "    if typereg == 'RF':\n",
        "        prediction_Y2 = RF(X,Y,X_2,labels_=positions,typedist_=typedist,verbose_=verbose)\n",
        "\n",
        "    ## -- OLS with Stepwise --\n",
        "    if typereg == 'OLSstep':\n",
        "        prediction_Y2 = OLSstep(X,Y,X_2, pi_step_=0.001,verbose_=verbose)\n",
        "\n",
        "    ## -- Gradiant boosting regression --\n",
        "    if typereg == 'Boosting':\n",
        "        prediction_Y2 = Boosting(X, Y, X_2,typedist_=typedist,verbose_=verbose)   \n",
        "\n",
        "    ## -- Bagging regression --\n",
        "    if typereg == 'Bagging':\n",
        "        prediction_Y2 = Bagging(X, Y, X_2,typedist_=typedist,verbose_=verbose)   \n",
        "\n",
        "    ## -- Linear regression --\n",
        "    if typereg == 'LinearReg':\n",
        "        prediction_Y2 = LinearReg(X, Y, X_2,typedist_=typedist,verbose_=verbose)   \n",
        "\n",
        "    ## -- Ada boosting --\n",
        "    if typereg == 'AdaBoost':\n",
        "        prediction_Y2 = AdaBoost(X, Y, X_2,typedist_=typedist,verbose_=verbose)  \n",
        "\n",
        "    ## -- Bayesian Ridge --\n",
        "    if typereg == 'BayesRidge':\n",
        "        prediction_Y2 = BayesRidge(X, Y, X_2,typedist_=typedist,verbose_=verbose)  \n",
        "\n",
        "    ## -- Lasso regression --\n",
        "    if typereg == 'LassoReg':\n",
        "        prediction_Y2 = LassoReg(X, Y, X_2,typedist_=typedist,verbose_=verbose)   \n",
        "\n",
        "    ## -- Ridge regression --\n",
        "    if typereg == 'RidgeReg':\n",
        "        prediction_Y2 = RidgeReg(X, Y, X_2,typedist_=typedist,verbose_=verbose)\n",
        "\n",
        "    ## -- PLS Regression --\n",
        "    if typereg == 'PLS':\n",
        "        prediction_Y2 = PLS(X, Y, X_2,n_components=n_components,typedist_=typedist,verbose_=verbose)\n",
        "\n",
        "    ## -- PCA Regression --\n",
        "    if typereg == 'PCR':\n",
        "        prediction_Y2 = PCR(X, Y, X_2,n_components=n_components,typedist_=typedist,verbose_=verbose)\n",
        "\n",
        "    ## -- Voting regression with ensemble models -- \n",
        "    if typereg == 'VotingEnsemble':\n",
        "        prediction_Y2 = VotingEnsemble(X,Y,X_2,verbose_=verbose)\n",
        "\n",
        "    ## -- Voting regression with linear model -- \n",
        "    if typereg == 'VotingLinear':\n",
        "        prediction_Y2 = VotingLinear(X,Y,X_2,verbose_=verbose)\n",
        "        \n",
        "    ## -- Random forrest regression with GridSearchCV--\n",
        "    if typereg == 'AutoRF':\n",
        "        prediction_Y2 = AutoRF(X, Y, X_2,labels_=positions,typedist_=typedist,verbose_=verbose)        \n",
        "\n",
        "    if verbose==True:\n",
        "        print_serie2(neighbors2,prediction_Y2, 'Pronóstico - ' + typedist+' - ' + typereg ,'Demanda','Tiempo',8,5,'fig_t16_Y2_'+typedist+'_'+typereg)\n",
        "\n",
        "    t_reg = time.time() - t_sel - t_o\n",
        "    #if typedist=='pearson':\n",
        "    #  t_sel =2.2038\n",
        "    #if typedist=='euclidian':\n",
        "    #  t_sel =23.4583\n",
        "    fail_=False\n",
        "    if len(prediction_Y2) == 0:\n",
        "        prediction_Y2=[serie[-1]] * vsele\n",
        "        fail_=True\n",
        "        print(\">>> analogo_knn: Pronóstico no calculado.\")\n",
        "\n",
        "    ## Dibujamos un ejemplo de espacio análogo\n",
        "    if False:\n",
        "        fig, ax = plt.subplots(figsize=(8,5))\n",
        "        ax.legend(['First line', 'Second line'])\n",
        "        serie1=serie[positions[0]:positions[0]+2*vsele]\n",
        "        serie2=serie[positions[1]:positions[1]+2*vsele]\n",
        "        serie3=serie[positions[2]:positions[2]+2*vsele]\n",
        "        serie4=serie[positions[3]:positions[3]+2*vsele]\n",
        "        serie5=serie[positions[4]:positions[4]+2*vsele]\n",
        "        serie6=serie[positions[5]:positions[5]+2*vsele]\n",
        "        ax.plot(serie1, label='X$_1$')\n",
        "        ax.plot(serie2, label='X$_2$')\n",
        "        ax.plot(serie3, label='X$_3$')\n",
        "        ax.plot(serie4, label='X$_4$')\n",
        "        ax.plot(serie5, label='X$_5$')\n",
        "        ax.plot(serie6,label='X$_6$')\n",
        "        ax.plot(Y,label='Y', linewidth=3, color='r')   \n",
        "        c = np.concatenate((Y,prediction_Y2), axis=0)   \n",
        "        ax.plot(c, label='$Y\\'$', linewidth=3, color='r',linestyle='--')      \n",
        "        plt.legend()\n",
        "        plt.axvline(x = vsele,linestyle='-.') # '-', '--', '-.', ':',\n",
        "        ax.set(xlabel='time (5 min)', ylabel='demand (MW)') #title='High correlation windows'\n",
        "        ax.grid()\n",
        "\n",
        "        fig.savefig('test'+str(random.randint(1,30000))+'.pdf')\n",
        "        plt.show()\n",
        "\n",
        "    return prediction_Y2, t_sel, t_reg, fail_"
      ],
      "metadata": {
        "id": "yz9CDOHQnx0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Pruebas unitarias del método análogo\n",
        "\n"
      ],
      "metadata": {
        "id": "UUWwvdYNRGyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Controla las pruebas unitarias al modelo análogo\n",
        "flag_unit_test =False"
      ],
      "metadata": {
        "id": "-cncnslXLCVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ... (time serie) ... window.window.window.window.window.window.window.window.window.window.window.\n",
        "##                                           |                                               |       |\n",
        "##                                           |                                               |       |\n",
        "##                                           |<------------------ vsele -------------------->|<vpred>|\n",
        "##                                                       (selection and regression)        (prediction)\n",
        "##                                                                                                 \n",
        "##|<---------------------------------------- train ----------------------------------------->| <test>|\n",
        "\n",
        "## Parámetros del método Análogo\n",
        "if flag_unit_test:\n",
        "    periods       = 288       ## Número de periodos en una ventana\n",
        "    periods_sele  = 1         ## Número de ventanas de selección\n",
        "    periods_pred  = 1         ## Número de ventanas de pronóstico\n",
        "\n",
        "    vsele = periods * periods_sele ## Número de periodos para selección\n",
        "    vpred = periods * periods_pred ## Número de periodos para pronóstico\n",
        "\n",
        "    k   = 6   ## k:   número de vecinos mas cercanos\n",
        "    tol = 0.8 ## tol: porcentaje de tolerancia de cercania entre ventanas\n",
        "\n",
        "    ## Dibujamos la ventana de histórico y los datos de prueba  \n",
        "    series = []; vacia = []; n = 2\n",
        "    series.append(serie[len(serie)-n*vsele : len(serie)-vsele+vpred])\n",
        "    series.append(serie[len(serie)-n*vsele : len(serie)-vsele])\n",
        "    print_serie2(serie_=series,prototipo_=vacia , title_='Serie de demanda eléctrica', ytitle_='Demanda (estandarizada)',xtitle_='Tiempo', sizex_=20, sizey_=6, namefile_='fig_t15_demanda')\n",
        "\n",
        "    ## Definimos la ventana de prueba y entrenamiento\n",
        "    X_train = serie[0 : len(serie) - vpred]\n",
        "    y_test  = serie[len(serie) - vpred : len(serie)]\n",
        "\n",
        "    ## Guardamos todos los resultados aquí\n",
        "    dferror = pd.DataFrame(columns=['distance','regression','MAE','MAPE','timesel','timereg'])\n",
        "\n",
        "    ## Bandera que activa todos los pronósticos\n",
        "    debug  = 0\n",
        "    active = 0"
      ],
      "metadata": {
        "id": "WapmM09KUQBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if flag_unit_test:\n",
        "  distances = ['pearson', 'euclidian']\n",
        "  methods = ['RF', 'OLSstep', 'Boosting', 'Bagging', 'LinearReg', 'AdaBoost', 'LassoReg', 'RidgeReg', 'PLS', 'PCR', 'VotingEnsemble', 'VotingLinear'] # AutoRF, 'BayesRidge'\n",
        "  preds = []\n",
        "  for d in distances:\n",
        "    for m in methods:\n",
        "      pred_, t_sel_, t_reg_ = analogo_knn(X_train, vsele=vsele, k=k, tol=tol, typedist=d, typereg=m, verbose=False)\n",
        "      preds.append({\n",
        "          'distance' : d,\n",
        "          'method'   : m,\n",
        "          'y_cap'    : pred_\n",
        "      })\n",
        "      mae_  = mean_absolute_error(y_test, pred_[0:vpred])\n",
        "      mape_ = mean_absolute_percentage_error(y_test,pred_[0:vpred])\n",
        "      dferror.loc[dferror.shape[0]] = [d, m, mae_, mape_,t_sel_,t_reg_]\n",
        "  preds = pd.DataFrame(preds)\n",
        "  \n",
        "  display(dferror.head())\n",
        "  display(preds.head())\n",
        "  dferror[\"time\"]         = dferror[\"timereg\"] + dferror[\"timesel\"]\n",
        "  dferror[\"invtime\"]      = 1 / ( dferror[\"timereg\"] + dferror[\"timesel\"])\n",
        "  dferror[\"invtime/MAPE\"] = dferror[\"invtime\"] / dferror[\"MAPE\"]\n",
        "  dferror[\"invtime/MAE\"]  = dferror[\"invtime\"] / dferror[\"MAE\"]\n",
        "  dferror.sort_values(\"invtime/MAE\",ascending=False)\n",
        "  dferror.sort_values(\"invtime/MAPE\",ascending=False)\n",
        "  dferror.sort_values(\"MAPE\",ascending=True)\n",
        "  dferror.head()"
      ],
      "metadata": {
        "id": "NvfOAP9itrtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if flag_unit_test:  \n",
        "  sel1 = ['R', 'Boosting', 'Bagging', 'AdaBoost', 'VotingEnsemble']\n",
        "  sel2 = ['OLSstep', 'LassoReg', 'RidgeReg', 'PLS', 'PCR', 'VotingLinear']\n",
        "\n",
        "  # cmap para colores distintos para cada distancia-método\n",
        "\n",
        "  for d in distances:\n",
        "    plt.figure(figsize=(25,8))\n",
        "    plt.plot(y_test, '.', label= '$Y_{\\\\beta+2}$')\n",
        "    for r in preds.iterrows():\n",
        "      values = r[1]\n",
        "      if values.distance == d and values.method in sel1:\n",
        "        plt.plot(values.y_cap, label = 'Y_{' + values.distance + ',' + values.method + '}')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "  for d in distances:\n",
        "    plt.figure(figsize=(25,8))\n",
        "    plt.plot(y_test, '.', label= '$Y_{\\\\beta+2}$')\n",
        "    for r in preds.iterrows():\n",
        "      values = r[1]\n",
        "      if values.distance == d and values.method in sel2:\n",
        "        plt.plot(values.y_cap, label = '$Y_{' + values.distance + ',' + values.method + '}$')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "L8wmhXZC1rnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ETL 5 minutes"
      ],
      "metadata": {
        "id": "eyRgIpSPswGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://drive.google.com/drive/folders/1Gj3XK9kM-lE18uBMe3qrZOGEm8yAI8i9\n",
        "#https://www.codegrepper.com/code-examples/python/how+to+read+csv+file+from+google+drive+on+google+colab+\n",
        "path = 'https://drive.google.com/uc?export=download&id=' \n",
        "URL  = 'https://drive.google.com/file/d/1057_dPk6rIZgXVku8kmZjq3m8WQNkJZb/view?usp=sharing'\n",
        "df = pd.read_csv(path+URL.split('/')[-2],usecols=[0,1,2,3,4,5,6,7],names=['Date','SERIE1','SERIE2','SERIE3','SERIE4','SERIE5','SERIE6','SERIE7'],\n",
        "                 dtype={'Date':str,'SERIE1':float,'SERIE2':float,'SERIE3':float,'SERIE4':float,'SERIE5':float,'SERIE6':float,'SERIE7':float}) #names=['CLVUNI','TYPE','NODE'], usecols=[1,2,3,4,5,6,]\n",
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "1e612CLOjx1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Date'] = pd.to_datetime(df['Date'],format= '%d-%m-%Y %H:%M' ) #.dt.date https://pandas.pydata.org/docs/reference/api/pandas.Series.dt.date.html\n",
        "df = df.set_index('Date')"
      ],
      "metadata": {
        "id": "8Xn_AmM-dL2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_5 = df.resample('5T').mean()\n",
        "df_5.head()"
      ],
      "metadata": {
        "id": "OGs5KlnZrPg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Se imprimen las series de tiempo\n",
        "if  False:\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(df_5.SERIE4, label='SERIE4')\n",
        "    ax.set(xlabel='time (5 min)', ylabel='demand (MW)') #title='Real vs Forecasting'\n",
        "    ax.grid()         \n",
        "    plt.legend()\n",
        "    plt.xticks(rotation=90)\n",
        "    fig.savefig(\"series.pdf\")\n",
        "    plt.show()\n",
        "\n",
        "    fig, axs = plt.subplots(2, 2,figsize=(8, 5))\n",
        "    axs[0, 0].plot(df_5.SERIE1, label='Series 1')\n",
        "    axs[0, 0].set(ylabel='demand (MW)') \n",
        "    axs[0, 0].legend(loc=\"upper right\")\n",
        "    axs[0, 1].plot(df_5.SERIE2, label='Series 2')\n",
        "    axs[0, 1].plot(df_5.SERIE4, label='Series 4')\n",
        "    axs[0, 1].legend(loc=\"upper right\")\n",
        "    axs[1, 0].plot(df_5.SERIE3, label='Series 3')\n",
        "    axs[1, 0].plot(df_5.SERIE5, label='Series 5')\n",
        "    axs[1, 0].set(ylabel='demand (MW)')\n",
        "    axs[1, 0].legend(loc=\"upper right\")\n",
        "    axs[1, 1].plot(df_5.SERIE6, label='Series 6')\n",
        "    axs[1, 1].plot(df_5.SERIE7, label='Series 7')\n",
        "    axs[1, 1].legend(loc=\"upper right\")\n",
        "    axs[1, 1].tick_params(axis=\"x\",labelsize=12,rotation=90) \n",
        "    axs[0, 1].tick_params(axis=\"x\",labelsize=12,rotation=90) \n",
        "    axs[1, 1].tick_params(axis=\"x\",labelsize=12,rotation=90) \n",
        "    axs[0, 0].tick_params(axis=\"x\",labelsize=12,rotation=90) \n",
        "    plt.sca(axs[1, 0])\n",
        "    plt.xticks(rotation=90,size=11)\n",
        "    plt.sca(axs[1, 1])\n",
        "    plt.xticks(rotation=90,size=11)\n",
        "\n",
        "    #for ax in axs.flat:\n",
        "    #    ax.set(xlabel='x-label', ylabel='y-label')\n",
        "\n",
        "    fig.tight_layout()\n",
        "    for ax in axs.flat:\n",
        "        ax.label_outer()\n",
        "    #for ax in axs.flat:\n",
        "    #    ax.set_xticklabels([])\n",
        "    #    ax.set_yticklabels([])\n",
        "\n",
        "    plt.subplots_adjust(wspace=0, hspace=0)\n",
        "\n",
        "    fig.savefig(\"series.pdf\")   \n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "3uAhJi9GGqcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if  False:\n",
        "    i = 0\n",
        "    j = 0\n",
        "    cmap = plt.cm.Dark2 #tab20 https://matplotlib.org/stable/gallery/color/colormap_reference.html\n",
        "    fig, ax = plt.subplots(figsize=(20,14))\n",
        "    for ts in df_5:\n",
        "      max_t = df_5[ts].max()\n",
        "      plt.plot(df_5[ts].index, df_5[ts] / max_t + j, c = cmap(i), label='Series '+str(i+1),alpha=.9)\n",
        "      i += 1\n",
        "      j -= 1\n",
        "    plt.yticks([])\n",
        "    plt.xticks(rotation=90, fontsize=16)\n",
        "    plt.legend(bbox_to_anchor=(1, 0.8), prop={'size': 16})\n",
        "    plt.savefig('series.pdf')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "u7HzIcQz8gyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Un pronóstico persistente multiperiodo"
      ],
      "metadata": {
        "id": "zAb715eJZfTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from numpy.lib.function_base import append\n",
        "def persistent(serie,n=1):\n",
        "    out=[]\n",
        "    for i in range(n):        \n",
        "        out.append(serie[len(serie)-1])\n",
        "    return(out)"
      ],
      "metadata": {
        "id": "qy00ObHKgeyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparamos cross-validation para pruebas\n",
        "\n",
        "Usaremos la metodología propuesta por Rob Hydman [cross-validation in time series](https://robjhyndman.com/hyndsight/tscv/#:~:text=Time%20series%20cross%2Dvalidation,used%20in%20constructing%20the%20forecast.\n",
        ")\n"
      ],
      "metadata": {
        "id": "HzBO6YghZoin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## https://robjhyndman.com/hyndsight/tscv/#:~:text=Time%20series%20cross%2Dvalidation,used%20in%20constructing%20the%20forecast.\n",
        "## ... year 2010                                                                                                    year 2011\n",
        "## ... january···february···march···april···may···june···july···august···september···october···november···december···january···february···march···april···may···june···july···august···september···october···november\n",
        "##    |                                   |      |\n",
        "##    |<------------- train ------------->|<test>|\n",
        "##    |<-january·february···march···april->|<may>|\n",
        "##            |<-february···march···april···may->|<june>|\n",
        "##                      |<- march···april···may···june ->|<-july->|\n",
        "##                              |<- april···may···june···july->|<-august->|\n",
        "##                                     |<-- may···june···july···august->|<-september->|\n",
        "##                                             |<-june···july···august···september->|<-october->|\n",
        "##                                                    |<-july···august···september···october->|<-november->|\n",
        "##                                                           |<-august···september···october···november->|<-december->| ...\n",
        "\n",
        "df_5_month = df_5.copy()\n",
        "df_5_month.reset_index(inplace=True)\n",
        "# https://stackoverflow.com/a/25149272\n",
        "df_5_month['month'] = df_5_month['Date'].dt.month\n",
        "df_5_month['year'] = df_5_month['Date'].dt.year\n",
        "df_5_month = df_5_month.drop_duplicates(['month', 'year'])\n",
        "df_5_month.index"
      ],
      "metadata": {
        "id": "MLECRwaNNpt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Obtenemos los \n",
        "month_i = df_5_month.index\n",
        "r = range(len(month_i) - 5)\n",
        "tuplas = []\n",
        "for i in r:\n",
        "  tuplas.append((\n",
        "    # 1 ene     30 abril            31 may\n",
        "    month_i[i], month_i[i + 4] - 1, month_i[i + 5] - 1\n",
        "  ))\n",
        "tuplas.append((157248, 192671, 200638))\n",
        "tuplas"
      ],
      "metadata": {
        "id": "uJt75hF6PILw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Función que regresa días"
      ],
      "metadata": {
        "id": "MRCcdDX3Zbzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_index_day(df,day,month,year):\n",
        "    df_i = df.reset_index()   ## Se agregan índices enteros.\n",
        "    idxo=df_i[(df_i.Date.dt.day==day)&(df_i.Date.dt.month==month)&(df_i.Date.dt.year==year)].iloc[ 0]\n",
        "    idxf=df_i[(df_i.Date.dt.day==day)&(df_i.Date.dt.month==month)&(df_i.Date.dt.year==year)].iloc[-1]\n",
        "    return(idxo.name,idxf.name)"
      ],
      "metadata": {
        "id": "oa_qq8mqZdG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "serie='SERIE1'\n",
        "print(get_index_day(df_5[serie],10,5,2010)) ## día de las madres 2010\n",
        "print(get_index_day(df_5[serie],10,5,2011)) ## día de las madres 2011"
      ],
      "metadata": {
        "id": "vpS7uXX2Y6ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extraemos la primera serie"
      ],
      "metadata": {
        "id": "0IeQpOpQYDOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "serie1 = df_5.SERIE1.tolist() ## Extraemos la primera serie\n",
        "plt.figure(figsize=(20, 4))\n",
        "plt.plot(serie1)"
      ],
      "metadata": {
        "id": "iOJ9pmphcnqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Meta-función básica de pronóstico de un periodo (persistente)\n",
        "\n"
      ],
      "metadata": {
        "id": "Bt2p4gGLcsva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# >>> provisional para pruebas preliminares (-comentar-)\n",
        "\n",
        "positions_test= [(0, 34559, 43487)]    ## Predecir mayo\n",
        "positions_test= [(8928, 43487, 43587)] ## Predecir junio\n"
      ],
      "metadata": {
        "id": "oFyvL1qYjah9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ... time serie·time-serie·time-serie·time-serie·time-serie·time-serie·time-serie·time-serie·time-serie·time-serie·time-serie ...\n",
        "##                |                                               |           |       |\n",
        "##                |                                               |<---n_p--->|       |\n",
        "##                |<------------------ train -------------------->|<-------test------>|\n",
        "##                to                                              tt                 tf \n",
        "n_p = 1  ## Numero de periodos de pronóstico por paso\n",
        "forecast_ = []\n",
        "time_     = []\n",
        "for to,tt,tf in positions_test:\n",
        "    j=0\n",
        "    for i in range(tt,tf,n_p):\n",
        "        t_o = time.time()\n",
        "        forecast_=forecast_+persistent(serie1[to+j:tt+j],n_p)\n",
        "        time_.append(time.time() - t_o)\n",
        "        j=j+n_p"
      ],
      "metadata": {
        "id": "t8g7sCWvckrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ = []\n",
        "for to,tt,tf in positions_test:\n",
        "      test_ = test_+serie1[tt:tf]\n",
        "print(len(forecast_))\n",
        "print(len(test_))"
      ],
      "metadata": {
        "id": "fZHSFwckkZoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mae_p  = mean_absolute_error(test_,forecast_)\n",
        "mape_p = mean_absolute_percentage_error(test_,forecast_)\n",
        "bias_p = bias.bias(test_,forecast_)\n",
        "print('mae_p=',mae_p,'mape_p=',mape_p,'bias=',bias_p)"
      ],
      "metadata": {
        "id": "TPZT-ey8tSY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Meta-función básica de n periodos (persistente)\n",
        "Se usan 30 periodos equivalentes a dos horas y media en intervalos de cinco minutos\n"
      ],
      "metadata": {
        "id": "h1KDNauA1VZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_p = 30 ## Numero de periodos por salto\n",
        "forecast2_ = []\n",
        "time2_     = []\n",
        "for to,tt,tf in positions_test:\n",
        "    j=0\n",
        "    for i in range(tt,tf,n_p):\n",
        "        t_o = time.time()\n",
        "        forecast2_=forecast2_+persistent(serie1[to+j:tt+j],n_p)\n",
        "        time2_.append(time.time() - t_o)\n",
        "        j=j+n_p\n",
        "    modu=(tf-tt)%n_p\n",
        "    if modu > 0:\n",
        "        forecast2_=forecast2_[0:(+modu-n_p)]\n",
        "            "
      ],
      "metadata": {
        "id": "hnnxqwMS1aet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(forecast2_))\n",
        "print(len(test_))"
      ],
      "metadata": {
        "id": "mqavnWO32Hqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gráficas"
      ],
      "metadata": {
        "id": "uiI_jXjy2Hfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(27,5))\n",
        "ax.plot(test_[0:35892],      label='Actual')\n",
        "ax.plot(forecast_[0:35892],  label='Persistence')\n",
        "ax.plot(forecast2_[0:35892], label='Persistence multi')\n",
        "ax.set(xlabel='time (5 min)', ylabel='demand (MW)')\n",
        "ax.grid()\n",
        "plt.legend()\n",
        "fig.savefig(\"test.pdf\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-GjtRaP845e2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mae_mp  = mean_absolute_error(test_,forecast2_)\n",
        "mape_mp = mean_absolute_percentage_error(test_,forecast2_)\n",
        "bias_mp  = bias.bias(test_,forecast2_)\n",
        "print('mae_mp=',mae_mp,'mape_mp=',mape_mp,'bias=',bias_mp)"
      ],
      "metadata": {
        "id": "0jtzx-yvkeE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Meta-función básica de pronóstico de un periodo (análogo)\n",
        "\n"
      ],
      "metadata": {
        "id": "plmnZ0dhaizG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## ... time serie·time-serie·time-serie·time-serie·time-serie·time-serie·time-serie·time-serie·time-serie·time-serie·time-serie ...\n",
        "##                |                                               |           |       |\n",
        "##                |                                               |<---n_p--->|       |\n",
        "##                |<------------------ train -------------------->|<-------test------>|\n",
        "##                to                  ...j...                     tt                 tf \n",
        "n_p = 1        ## Numero de periodos por salto\n",
        "forecastAn_   = []\n",
        "forecastAnMA_ = []\n",
        "time3_        = []\n",
        "n=0\n",
        "nfail=0\n",
        "fail_=False\n",
        "for to,tt,tf in positions_test:\n",
        "    j=0\n",
        "    for i in range(tt,tf,n_p):\n",
        "        ## Parámetros del método Análogo\n",
        "        vsele = 288 ## Número de periodos en una ventana\n",
        "        k     = 6   ## k:   número de vecinos mas cercanos\n",
        "        tol   = 0.8 ## tol: porcentaje de tolerancia de cercania entre vecinos\n",
        "        d     = 'pearson'\n",
        "        m     = 'OLSstep'\n",
        "        X_train = numpy.array(serie1[to+j:tt+j])\n",
        "\n",
        "        t_o = time.time()\n",
        "        try:\n",
        "            pred_, t_sel_, t_reg_, fail_ = analogo_knn(X_train, vsele=vsele, k=k, tol=tol, typedist=d, typereg=m, verbose=False)\n",
        "        except:\n",
        "            print(\"!!! Error en la posicion:\",tt+j)\n",
        "        if fail_==True:              \n",
        "            nfail = nfail + 1\n",
        "            print(\">>> Pronóstico persistente en la posicion:\",tt+j)\n",
        "\n",
        "        #forecastAn_.append(pred_[0])\n",
        "        pred_list  = pred_.tolist() \n",
        "        forecastAn_ = forecastAn_ + pred_list[0:n_p]\n",
        "        if j >= 20 :\n",
        "            a = -min(j,vsele)\n",
        "            b = -n_p\n",
        "            array1 = np.array(forecastAn_[a:b])\n",
        "            c = tt +j -min(j,vsele)\n",
        "            d = tt +j -n_p \n",
        "            array2 = np.array(serie1[c:d])                        \n",
        "            epsilon = np.subtract(array1,array2) ## Errores del primer pronóstico\n",
        "            ar = AutoReg(epsilon, lags=1).fit()           \n",
        "            delta = ar.forecast(1)\n",
        "            forecastAnMA_ = forecastAnMA_ + [pred_[0] - delta]\n",
        "        else:\n",
        "            forecastAnMA_ = forecastAn_\n",
        "\n",
        "        time3_.append(time.time() - t_o)\n",
        "        j=j+n_p\n",
        "\n",
        "    modu=(tf-tt)%n_p\n",
        "\n",
        "    if modu > 0:\n",
        "        forecastAn_  =forecastAn_[0:(+modu-n_p)]\n",
        "        forecastAnMA_=forecastAnMA_[0:(+modu-n_p)]\n",
        "        print(len(forecastAn_)); len(type(forecastAnMA_))\n",
        "\n",
        "print('>>> Número de pronósticos no calculados',nfail)            "
      ],
      "metadata": {
        "id": "GkwIye0OaizG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(forecastAn_))   \n",
        "print(len(forecastAnMA_)) "
      ],
      "metadata": {
        "id": "b89AAnheHtKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "date_i = df_5.reset_index().iloc[tt].Date"
      ],
      "metadata": {
        "id": "R8cbrTK7WJfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_results = pd.DataFrame({\n",
        "  'datetime' : pd.date_range(date_i, periods=len(test_), freq=\"5T\"),\n",
        "  'Actual' : test_, \n",
        "  'Persistence' : forecast_, \n",
        "  'An Pearson+OLS' : forecastAn_, \n",
        "  'AnMA Pearson+OLS' : forecastAnMA_\n",
        "})\n",
        "df_results = df_results.set_index('datetime')\n",
        "plt.figure(figsize=(16, 4))\n",
        "for c in df_results:\n",
        "  if c == 'Actual':\n",
        "    plt.plot(df_results[c], '--', label = c)\n",
        "  else:\n",
        "    plt.plot(df_results[c], label = c, alpha=0.8)\n",
        "plt.legend()\n",
        "plt.savefig(\"test2.pdf\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sqGToqb9XIfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(28,3))\n",
        "ax.plot(test_,        label='Actual') \n",
        "#ax.plot(forecast_ ,  label='Persistence')\n",
        "ax.plot(forecastAn_,  label='An Pearson+OLS')\n",
        "ax.plot(forecastAnMA_,label='AnMA Pearson+OLS')\n",
        "ax.set(xlabel='time (5 min)', ylabel='demand (MW)')\n",
        "ax.grid()\n",
        "plt.legend()\n",
        "#plt.axis([100,  200, 6000, 8000])\n",
        "fig.savefig(\"bias.pdf\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QUtWf1GfUzie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=0; b=100\n",
        "mae_p  = mean_absolute_error(test_[a:b],forecast_[a:b])\n",
        "mape_p = mean_absolute_percentage_error(test_[a:b],forecast_[a:b])\n",
        "bias_p = bias.bias(test_[a:b],forecast_[a:b])\n",
        "print('mape_p=',mape_p,'mae_p=',mae_p,'bias_p=',bias_p)"
      ],
      "metadata": {
        "id": "q_L6tT-tfLKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mae_a  = mean_absolute_error(test_[a:b],forecastAn_[a:b])\n",
        "mape_a = mean_absolute_percentage_error(test_[a:b],forecastAn_[a:b])\n",
        "bias_a = bias.bias(test_[a:b],forecastAn_[a:b])\n",
        "print('mape_a=',mape_a,'mae_a=',mae_a,'bias_a=',bias_a)"
      ],
      "metadata": {
        "id": "24ulU9rKlYb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mae_ma  = mean_absolute_error(test_[a:b],forecastAnMA_[a:b])\n",
        "mape_ma = mean_absolute_percentage_error(test_[a:b],forecastAnMA_[a:b])\n",
        "bias_ma = bias.bias(test_[a:b],forecastAnMA_[a:b])\n",
        "print('mape_ma=',mape_ma,'mae_ma=',mae_ma,'bias_ma=',bias_ma)"
      ],
      "metadata": {
        "id": "67W4rxQzPfxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [diff(test_[a:b],forecast_[a:b]), diff(test_[a:b],forecast2_[a:b]), diff(test_[a:b],forecastAn_[a:b])]\n",
        "labels = ['Persistence', 'An\\n(Pearson+OLS)', 'AnMA\\n(Pearson+OLS)']\n",
        "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(6, 6))\n",
        "CYAN = '#76ced6' ; LILA = '#777bd4'; VERDE='#17cb49'; LETRASNARA ='#ff8000'; AZUL='#168fff'; OTROAZUL = \"b-\"; ROJO= \"#FF0000\"; # 'pink', 'lightblue', 'lightgreen',\n",
        "#plt.tick_params(colors = LETRASNARA, which='both')\n",
        "bplot1 = axes.boxplot(data,\n",
        "                      vert=True,          # vertical box alignment\n",
        "                      patch_artist=True,  # fill with color\n",
        "                      labels=labels,)     # will be used to label x-ticks\n",
        "#axes.set_title('Accuracy',fontsize='x-large',color = LETRASNARA)\n",
        "#plt.tick_params(colors = LETRASNARA, which='both')\n",
        "colors = ['pink', 'lightblue', 'lightgreen',LILA, AZUL] # fill with colors\n",
        "for patch, color in zip(bplot1['boxes'], colors):\n",
        "    patch.set_facecolor(color)\n",
        "axes.yaxis.grid(True) # adding horizontal grid lines\n",
        "#axes.set_xlabel('Configuraciones de red',fontsize='large',color = LETRASNARA)\n",
        "axes.set_ylabel('')\n",
        "namefile = 'boxplot.pdf'\n",
        "plt.savefig(namefile, transparent=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gJTrd655bWox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Tiempo medio de AnMA Pearson OLS en horas\n",
        "plt.plot(time3_,)\n",
        "((np.mean(time3_)*8723)/60)/60 "
      ],
      "metadata": {
        "id": "zqiC_AIINVJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_resultados = pd.DataFrame([\n",
        "  {\n",
        "    'method' : 'Persistence',\n",
        "    'mape' : mape_p,\n",
        "    'mae' : mae_p, # Conviene normalizar esto (o quitarlo porque normalizado es como el mape)\n",
        "    'bias' : bias_p # Conviene normalizar esto\n",
        "  },\n",
        "  {\n",
        "    'method' : 'An P+OLS',\n",
        "    'mape' : mape_a,\n",
        "    'mae' : mae_a, # Conviene normalizar esto (o quitarlo porque normalizado es como el mape)\n",
        "    'bias' : bias_a # Conviene normalizar esto\n",
        "  },\n",
        "  {\n",
        "    'method' : 'AnMA P+OLS',\n",
        "    'mape' : mape_ma,\n",
        "    'mae' : mae_ma, # Conviene normalizar esto (o quitarlo porque normalizado es como el mape)\n",
        "    'bias' : bias_p # Conviene normalizar esto\n",
        "  },\n",
        "  # etc.\n",
        "])\n",
        "categories = df_resultados.method\n",
        "N = len(categories)\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection=\"polar\")\n",
        "\n",
        "theta = np.arange(len(df_resultados) + 1) / float(len(df_resultados)) * 2 * np.pi\n",
        "\n",
        "values = df_resultados.mape.values\n",
        "values = np.append(values, values[0])\n",
        "values = values + abs(min(values))\n",
        "values = values / max(values)\n",
        "ax.plot(theta, values, label=\"MAPE\")\n",
        "\n",
        "values = df_resultados.bias.values\n",
        "values = np.append(values, values[0])\n",
        "values = values + abs(min(values))\n",
        "values = values / max(values)\n",
        "ax.plot(theta, values, label=\"Bias\")\n",
        "\n",
        "plt.xticks(theta[:-1], df_resultados.method, color='grey', size=12)\n",
        "plt.yticks([0.25, 0.50, 0.75], ['0.25', '0.50', '0.75'], color=\"grey\", size=9)\n",
        "ax.tick_params(pad=10) # to increase the distance of the labels to the plot\n",
        "\n",
        "plt.legend(loc='center right', bbox_to_anchor=(0.1, 0.15))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EYUOpNdxUl6i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}