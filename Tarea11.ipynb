{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tarea11.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOFG8PUCZe+uzgNMQAuSQf1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urieliram/statistical/blob/main/Tarea11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jl8U_TJwYnCt",
        "outputId": "20597d6a-de5d-45ca-a039-958b7c6ee8a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.8.0)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split #--------------splitting data into test and train\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from IPython.core.pylabtools import figsize\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%matplotlib inline\n",
        "!sudo pip3 install keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.layers import Dense, Conv1D, Flatten, MaxPooling1D\n",
        "from tensorflow.keras.layers import LocallyConnected2D\n",
        "from tensorflow.keras.layers import LocallyConnected1D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "print(tf.__version__)\n",
        "from matplotlib import pyplot as plt\n",
        "from random import randint\n",
        "\n",
        "import keras\n",
        "from   keras.models import Sequential\n",
        "from   keras.layers import Dense"
      ],
      "metadata": {
        "id": "03KT7DFyii20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Predicción de sobrecarga en grupos de líneas de transmisión.\n",
        "En esta sección se usará inferencia bayesiana para ajustar un modelo de regresión logística a datos de violación de flujo de potencia eléctrica en grupos de líneas de transmisión, que interconectan regiones eléctricas. La variable dependientes es de naturaleza binaria con un valor de uno si la línea presenta sobrecarga y cero si no. Las variables independientes son el flujo neto máximo y mínimo en la región eléctrica en un día y se calcula como la diferencia entre la demanda menos la generación en cada región."
      ],
      "metadata": {
        "id": "Oq5QAPZbZFIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('overload.csv')\n",
        "df = df.assign(const=1)\n",
        "dfy = df[['L3','L5','L6','L7','L14','L15','L22','L31','L38','L39','L51','L58','L65']] #Se escoge el número de la línea de transmisión\n",
        "dfx = df[['CEN','NES','NOR','NTE','OCC','ORI','PEN','CEN_min','NES_min','NOR_min','NTE_min','OCC_min','ORI_min','PEN_min']] ## Predictors\n",
        "\n",
        "X = dfx.to_numpy()\n",
        "#y = dfy.to_numpy()\n",
        "\n",
        "#Normalizing the data\n",
        "sc = StandardScaler()\n",
        "X = sc.fit_transform(X)\n",
        "\n",
        "#ohe = OneHotEncoder()\n",
        "#y = ohe.fit_transform(y).toarray()\n",
        "\n",
        "## Crea conjuntos de datos de entrenamiento y prueba\n",
        "#X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.7) #, random_state = 5\n",
        "\n",
        "#df.sample(5)\n",
        "#df.describe()\n",
        "# df.head()"
      ],
      "metadata": {
        "id": "YHm2TKzKJj1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación se enlista las configuraciones de red usadas en el libro (para resolver el problema *11.7 Example: ZIP Code Data*). \n",
        "*   Net-1: Sin capa oculta, equivalente a regresión logística multinomial.\n",
        "*   Net-2: Una capa oculta, 12 unidades ocultas totalmente conectadas.\n",
        "*   Net-3: Dos capas ocultas conectadas localmente.\n",
        "*   Net-4: Dos capas ocultas, conectadas localmente con peso compartido.\n",
        "*   Net-5: dos capas ocultas, conectadas localmente, dos niveles de peso compartido.\n",
        "\n",
        "Trataremos de replicar el ejercicio aplicando a nuestros datos. A diferencia del problema de 2 dimensiones del libro, usaremos redes para una sola dimensión para predecir un vector de binarios (sobrecarga o no de algunas líneas de tranmisión) a partir de un vector de datos reales (datos de demanda de las regiones eléctricas). También mencionan que todas las redes tienen unidades de salida sigmoidales y el ajuste es con mínimpos cuadrados."
      ],
      "metadata": {
        "id": "fyP3Pfx9i0bh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://towardsdatascience.com/building-our-first-neural-network-in-keras-bdc8abbc17f5\n",
        "## Neural network\n",
        "\n",
        "input_dim   = 14\n",
        "num_classes = 2 ## {salidas}\n",
        "\n",
        "model  = Sequential()\n",
        "\n",
        "'''\n",
        "## Net-1\n",
        "model.add(Dense(units = num_classes, input_dim = 14, activation='sigmoid' )) ## FUNCIONA !!! , activation='relu',\n",
        "nfig = 1\n",
        "'''\n",
        "\n",
        "\n",
        "## Net-2\n",
        "model.add(Dense(units=12, input_dim = input_dim,  activation ='sigmoid' )) ## FUNCIONA !!!\n",
        "model.add(Dense(units=num_classes,         activation ='sigmoid'))\n",
        "nfig = 2\n",
        "\n",
        "\n",
        "'''\n",
        "## Net-3\n",
        "input_dim = (14,1)\n",
        "input_ = Input(input_dim, name = 'the_input')\n",
        "layer1 = LocallyConnected1D(1, 2, strides= 2, activation= 'sigmoid', name = 'layer1')(input_)\n",
        "layer2 = LocallyConnected1D(1, 5, activation='sigmoid', name = 'layer2')(layer1)\n",
        "layer3 = Flatten(name='layer3')(layer2) \n",
        "output = Dense(units=num_classes, activation='sigmoid', name = 'output')(layer3)\n",
        "model = Model(inputs = input_, outputs = output)\n",
        "input_dim = np.expand_dims(input_dim, axis=0)\n",
        "nfig = 3\n",
        "'''\n",
        "\n",
        "'''\n",
        "## Net-4\n",
        "input_dim = (14,1)\n",
        "input_ = Input(input_dim, name = 'the_input')\n",
        "layer1 = Conv1D(filters=2, kernel_size=2, strides=2, activation='sigmoid', name='layer1')(input_) \n",
        "layer2 = LocallyConnected1D(1, 5, activation='sigmoid', name='layer2')(layer1)\n",
        "layer3 = Flatten(name='layer3')(layer2) \n",
        "output = Dense(units=num_classes, activation='sigmoid', name = 'output')(layer3)\n",
        "model = Model(inputs = input_, outputs = output)\n",
        "input_dim = np.expand_dims(input_dim, axis=0)\n",
        "nfig = 4\n",
        "\n",
        "'''\n",
        "'''\n",
        "## Net-5\n",
        "input_dim = (14,1)\n",
        "input_ = Input(input_dim, name = 'the_input')\n",
        "layer1 = Conv1D(2, 2, strides= 2, activation= 'sigmoid', name = 'layer1')(input_)\n",
        "layer2 = Conv1D(4, 5, activation='sigmoid', name = 'layer2')(layer1)\n",
        "layer3 = Flatten(name='layer3')(layer2) \n",
        "output = Dense(units=num_classes, activation='sigmoid', name = 'output')(layer3)\n",
        "model = Model(inputs = input_, outputs = output)\n",
        "input_dim = np.expand_dims(input_dim, axis=0)\n",
        "nfig = 5\n",
        "'''\n",
        "\n",
        "'''\n",
        "## LeNet5\n",
        "Hg = 200\n",
        "Lng = 80\n",
        "\n",
        "#Layer 1\n",
        "#Conv Layer 1\n",
        "input_dim = (14,1)\n",
        "model.add(Conv1D(filters = 6, kernel_size = 5, strides = 1, activation = 'relu', \n",
        "                 input_shape = input_dim ))\n",
        "#Pooling layer 1\n",
        "model.add(MaxPooling1D(pool_size = 2, strides = 2))\n",
        "#Layer 2\n",
        "#Conv Layer 2\n",
        "model.add(Conv1D(filters = 16, kernel_size = 5,strides = 1,activation = 'relu',\n",
        "                 input_shape = input_dim ))\n",
        "#Pooling Layer 2\n",
        "model.add(MaxPooling1D(pool_size = 2, strides = 2))\n",
        "\n",
        "#Flatten\n",
        "model.add(Flatten())\n",
        "#Layer 3\n",
        "#Fully connected layer 1\n",
        "model.add(Dense(units = 120, activation = 'relu'))\n",
        "#Layer 4\n",
        "#Fully connected layer 2\n",
        "model.add(Dense(units = 84, activation = 'relu'))\n",
        "#Layer 5\n",
        "#Output Layer\n",
        "model.add(Dense(units = 2, activation = 'sigmoid'))\n",
        "'''\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "fxGqkvDWi9_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#parametros https://www.tensorflow.org/api_docs/python/tf/keras/Model\n",
        "#RMSprop, categorical_crossentropy, adam, binary_crossentropy, categorical_crossentropy, sgd\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) "
      ],
      "metadata": {
        "id": "wogbeqm9i_J2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs     = 100 \n",
        "batch_size = 64\n",
        "verbose    = 0\n",
        "results    = []\n",
        "\n",
        "for item in ['L3','L5','L6','L7','L14','L15','L22','L31','L38','L39','L51','L58','L65']:\n",
        "#for item in dfy:\n",
        "    print(item)\n",
        "    dfy = df[[item]] #Se escoge el número de la línea de transmisión\n",
        "    y = dfy.to_numpy()    \n",
        "    ohe = OneHotEncoder()\n",
        "    y = ohe.fit_transform(y).toarray()\n",
        "    ## Crea conjuntos de datos de entrenamiento y prueba\n",
        "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.7) #, random_state = 5\n",
        "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose) \n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    #Converting predictions to label\n",
        "    pred = list()\n",
        "    for i in range(len(y_pred)):\n",
        "        pred.append(np.argmax(y_pred[i]))\n",
        "    #Converting one hot encoded test label to label\n",
        "    test = list()\n",
        "    for i in range(len(y_test)):\n",
        "        test.append(np.argmax(y_test[i]))\n",
        "\n",
        "\n",
        "    loss, accuracy  = model.evaluate(X_test, y_test)\n",
        "    #acu = accuracy_score(pred,test)\n",
        "    print('Accuracy of ',item,' is:', accuracy*100)\n",
        "    print('Loss of     ',item,' is:', loss*100)\n",
        "    results.append( (i,accuracy*100, loss*100 ))\n",
        "    \n",
        "    history = model.fit(X_train, y_train, validation_data = (X_test,y_test), epochs=epochs, batch_size=batch_size,verbose=verbose) #epochs =200 batch_size =64\n",
        "    #print(history.history.keys())\n",
        "\n",
        "    CYAN = '#76ced6' ; LILA = '#777bd4'; VERDE='#17cb49'; LETRASNARA ='#ff8000'; AZUL='#168fff'; OTROAZUL = \"b-\"; ROJO= \"#FF0000\";\n",
        "    fig, ax = plt.subplots(figsize=(6,6))\n",
        "    plt.plot(history.history['accuracy'], color = ROJO)\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('Exactitud del modelo',fontsize='x-large',color = LETRASNARA)\n",
        "    plt.ylabel('Exactitud',fontsize='large',color = LETRASNARA)\n",
        "    plt.xlabel('Epoch',fontsize='large',color = LETRASNARA)\n",
        "    plt.legend(['Entrenamiento', 'Prueba'], loc='upper left')\n",
        "    plt.tick_params(colors = LETRASNARA, which='both')\n",
        "    ax.spines['bottom'].set_color(LETRASNARA)\n",
        "    ax.spines['top'   ].set_color(LETRASNARA) \n",
        "    ax.spines['right' ].set_color(LETRASNARA)\n",
        "    ax.spines['left'  ].set_color(LETRASNARA)\n",
        "    namefile = 'fig_t11_' + str(nfig) + '_a'\n",
        "    plt.savefig(namefile, transparent=True)\n",
        "    plt.show()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6,6))\n",
        "    plt.plot(history.history['loss'], color = ROJO) \n",
        "    plt.plot(history.history['val_loss']) \n",
        "    plt.tick_params(colors = LETRASNARA, which='both')\n",
        "    ax.spines['bottom'].set_color(LETRASNARA)\n",
        "    ax.spines['top'   ].set_color(LETRASNARA) \n",
        "    ax.spines['right' ].set_color(LETRASNARA)\n",
        "    ax.spines['left'  ].set_color(LETRASNARA)\n",
        "    plt.title('Pérdida en el modelo',fontsize='x-large',color = LETRASNARA) \n",
        "    plt.ylabel('Pérdida',fontsize='large',color = LETRASNARA) \n",
        "    plt.xlabel('Epoch',fontsize='large',color = LETRASNARA) \n",
        "    plt.legend(['Entrenamiento', 'Prueba'], loc='upper left') \n",
        "    namefile = 'fig_t11_' + str(nfig) + '_b'\n",
        "    plt.savefig(namefile, transparent=True)\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k54yEdixjce5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results)"
      ],
      "metadata": {
        "id": "akfZw9TIBv27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Conclusión:** \n",
        "Hemos utilizado la API funcional de **Keras** para implementar diferentes estructuras de redes neuronales para predicción de sobrecarga en líneas de transmisión de acuerdo a la demanda en las regiones.\n",
        "\n",
        "En los resultados podemos observar que mientras la complejidad de la estructura en las redes Net-1, Net-2 y Net-2 aumenta, la exactutud del modelo mejora. Sin embargo, en el caso de las redes Net-4 y Net-5 la exactitud del modelo fue de cero, la característica común de estas dos redes son que comparten pesos entre capas. Por lo que podemos suponer que para nuestros datos, esta estrategia puede no ser adecuada.\n",
        "En general el desempeño de las cinco redes fue muy malo para nuestros datos, para poder obtener mejores resultados, podriamos cambiar las arquitecturas,  parámetros e incluso optimizadores (para esta tarea se usaron estrictamente las estructuras y parpametros del libro)."
      ],
      "metadata": {
        "id": "Ql3ADEdWZY3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Losses KERAS](https://keras.io/api/losses/)\n",
        "\n",
        "[Ejemplos KERAS](https://keras.io/examples/)\n",
        "\n",
        "[Model class API](https://faroit.com/keras-docs/1.2.2/models/model/)\n",
        "\n",
        "[The Functional API TUTORIAL](https://keras.io/guides/functional_api/)\n",
        "\n",
        "[https://gist.github.com/jkleint/1d878d0401b28b281eb75016ed29f2ee](https://gist.github.com/jkleint/1d878d0401b28b281eb75016ed29f2ee)\n",
        "\n",
        "[Introduction to 1D Convolutional Neural Networks in Keras for Time Sequences](https://blog.goodaudience.com/introduction-to-1d-convolutional-neural-networks-in-keras-for-time-sequences-3a7ff801a2cf)\n",
        "\n",
        "[1D Convolutional Neural Network Models for Human Activity Recognition](https://machinelearningmastery.com/cnn-models-for-human-activity-recognition-time-series-classification/)"
      ],
      "metadata": {
        "id": "gyYCVhnKqHRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://stackoverflow.com/questions/49161174/tensorflow-logits-and-labels-must-have-the-same-first-dimension\n",
        "\n",
        "#n_class = 2\n",
        "#n_features = 14\n",
        "#inputs = keras.Input(shape=(n_features,))\n",
        "\n",
        "#inputs.shape\n",
        "#inputs.dtype\n",
        "\n",
        "#dense = layers.Dense(14, activation = \"sigmoid\")\n",
        "#x = dense(inputs)\n",
        "\n",
        "#x = layers.Dense(14, activation=\"sigmoid\")(x)\n",
        "#outputs = layers.Dense(14)(x)\n",
        "\n",
        "#outputs = x\n",
        "\n",
        "#model = keras.Model(inputs=inputs, outputs=outputs, name = \"overload_model\")\n",
        "\n",
        "#np.expand_dims(inputs,axis=0)\n",
        "\n",
        "#model.summary()\n",
        "\n",
        "#keras.utils.plot_model(model, \"fig_t11_1.png\")\n",
        "#keras.utils.plot_model(model, \"fig_t11_2.png\", show_shapes=True)"
      ],
      "metadata": {
        "id": "z_MhV61PZEg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from keras.models import Model\n",
        "#from keras.layers import Input, Dense\n",
        "\n",
        "#a = Input(shape=(32,))\n",
        "#b = Dense(32)(a)\n",
        "#model = Model(Input=a, Output=b)\n",
        "#compile(self, optimizer, loss, metrics=None, loss_weights=None, sample_weight_mode=None)\n",
        "#fit(self, x, y, batch_size=32, nb_epoch=10, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0)"
      ],
      "metadata": {
        "id": "mFgH6kNqc5ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model.compile(\n",
        "#    loss      = keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
        "#    optimizer = keras.optimizers.RMSprop(),\n",
        "#    metrics=[\"accuracy\"],\n",
        "#)\n",
        "\n",
        "#history = model.fit(x_train, y_train, batch_size=14, epochs=2, validation_split=0.2)\n",
        "\n",
        "#test_scores = model.evaluate(x_test, y_test, verbose=2)\n",
        "#print(\"Test loss:\", test_scores[0])\n",
        "#print(\"Test accuracy:\", test_scores[1])"
      ],
      "metadata": {
        "id": "p45sI-QcSo0M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}