{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tarea5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNZNCvrKG1wNb/OUCVnoeoB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urieliram/statistical/blob/main/Tarea5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "import warnings\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV, LarsCV\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import scipy.stats\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.tools.eval_measures as bias\n",
        "import seaborn as sns\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "w3MO8TIfkMQF"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkremTHzBm9f",
        "outputId": "f9d969ef-944b-4c51-e231-7c5d2ddb9f1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A very bad model: -21619.4 [519.6  -2.7 151.4]\n",
            "12 13 mismatch\n",
            "1 1 ok\n",
            "4 4 ok\n",
            "-5 -5 ok\n",
            "3 3 ok\n",
            "0 0 ok\n",
            "0 0 ok\n",
            "0 1 mismatch\n",
            "-2 -1 mismatch\n",
            "0 0 ok\n",
            "4 1 mismatch\n",
            "0 -1 mismatch\n",
            "0 0 ok\n",
            "0 0 ok\n",
            "0 0 ok\n",
            "2 2 ok\n",
            "A transformed model: -21619.4 [519.6  -2.7 151.4]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "np.set_printoptions(precision = 1)\n",
        "\n",
        "n = 50\n",
        "# assume these are the original features\n",
        "x1 = np.random.randint(size = n, low = 50, high = 100)\n",
        "x2 = np.random.randint(size = n, low = 2, high = 20)\n",
        "x3 = np.random.randint(size = n, low = 5, high = 50)\n",
        "\n",
        "# and this is the model with some gaussian noise\n",
        "y = 12 + x1 + 4 * x2 - 5 * x3 \\\n",
        "    + 3 * x1**2 \\\n",
        "    - 2 * np.sqrt(x2) \\\n",
        "    + 4 * np.log(x1) \\\n",
        "    + 2 * x1 * x3 \\\n",
        "    + np.random.normal(size = n, loc = 0, scale = 0.05)\n",
        "\n",
        "# first do regression as such\n",
        "X = np.column_stack((x1, x2, x3))\n",
        "\n",
        "raw = LinearRegression()\n",
        "raw.fit(X, y)\n",
        "print(f'A very bad model: {raw.intercept_:.1f}', np.round(raw.coef_, 1))\n",
        "\n",
        "# TRANSFORMATIONS\n",
        "# add squares of raw features\n",
        "tf1 = np.power(x1, 2)\n",
        "tf2 = np.power(x2, 2)\n",
        "tf3 = np.power(x3, 2)\n",
        "X = np.column_stack((X, tf1, tf2, tf3))\n",
        "# add roots\n",
        "tf4 = np.sqrt(x1)\n",
        "tf5 = np.sqrt(x2)\n",
        "tf6 = np.sqrt(x3)\n",
        "X = np.column_stack((X, tf4, tf5, tf6))\n",
        "# add logarithms\n",
        "tf7 = np.log(x1)\n",
        "tf8 = np.log(x2)\n",
        "tf9 = np.log(x3)\n",
        "X = np.column_stack((X, tf4, tf5, tf6))\n",
        "# add products\n",
        "tf10 = np.multiply(x1, x2)\n",
        "tf11 = np.multiply(x2, x3)\n",
        "tf12 = np.multiply(x1, x3)\n",
        "X = np.column_stack((X, tf10, tf11, tf12))\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "for (desired, obtained) in zip([12, 1, 4, -5, 3, 0, 0, 0, -2, 0, 4, 0, 0, 0, 0, 2],\n",
        "                                   [round(model.intercept_)] + list([int(i) for i in np.round(model.coef_)])):\n",
        "    print(desired, obtained, 'ok' if desired == obtained else 'mismatch')\n",
        "    \n",
        "print(f'A transformed model: {raw.intercept_:.1f}', np.round(raw.coef_, 1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#función que calcula la predicción Y[] de un X[] usando los parametros o coeficientes de regresión beta[]\n",
        "def pronostica(params,X): #params:parámetros de regresión; X:datos de regresores \n",
        "    y = []\n",
        "    i = 0\n",
        "    for rows in X:\n",
        "        y.append(np.matmul(params,X[i]))\n",
        "        i = i + 1\n",
        "    return(y)"
      ],
      "metadata": {
        "id": "xqJe3d3HhhVL"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dibuja_hist(df,colour,name,Xlabel,Ylabel,title):\n",
        "    plt.figure()\n",
        "    df.hist(column=0, bins=25, grid=False, figsize=(6,3), color=colour, zorder=2, rwidth=0.9)\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.xlabel(Xlabel)\n",
        "    plt.ylabel(Ylabel)\n",
        "    #https://stackoverflow.com/questions/9662995/matplotlib-change-title-and-colorbar-text-and-tick-colors/42020486\n",
        "    mytitle = plt.title(title) # get the title property handler   #plt.getp(title_obj)  \n",
        "    plt.setp(mytitle, color='#ff8000')                            # set the color of title\n",
        "    myax = plt.axes()   # get the axis property handler           # plt.getp(myax) print its propieties\n",
        "    myax.xaxis.label.set_color('#ff8000')\n",
        "    myax.yaxis.label.set_color('#ff8000')\n",
        "    myax.tick_params(colors='#ff8000', which='both')              # myax.spines['bottom'].set_color('yellow')\n",
        "    plt.savefig(name, transparent=True)\n",
        "    plt.show()                                                    # o plt.save_en_algún_formato()"
      ],
      "metadata": {
        "id": "KAYlAFp2ELGZ"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regresión lineal en predicción de demanda eléctrica\n",
        "\n",
        "A continuación aplicaremos técnicas de expansión de base aplicados a predicción de demanda eléctrica por regresión, tal como se hizo en el la [tarea 3](https://colab.research.google.com/github/urieliram/statistical/blob/main/Tarea3.ipynb#scrollTo=Ext-myaET2n5). Recordando que la variable independiente $X$ serán los datos de demanda del día anterior, y los datos independiente $Y$ serán los datos de días con una mayor correlación. Compararemos el desempeño de usar transformación variables y algunas tpecnicas de reducción de dimensiones como **stepwise**.\n",
        "\n",
        "Los datos usados en esta sección están disponibles en [demanda.csv](https://drive.google.com/file/d/1KpY2p4bfVEwGRh5tJjMx9QpH6SEwrUwH/view?usp=sharing)"
      ],
      "metadata": {
        "id": "rVCPvAT2kf9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('demanda.csv')"
      ],
      "metadata": {
        "id": "ymjPNdm3qKvY"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Procesamos datos de entrenamiento \n",
        "df1 = df.loc[df.iloc[:,19].isin(['T'])] ## 'T' = training set\n",
        "df2 = df1['Y']  \n",
        "df1 = df1[['X1','X2','X3','X4','X5','X6','X7','X8','X9','X10']] ## Regresores"
      ],
      "metadata": {
        "id": "dOt5Pvh236eH"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, cargamos los datos de prueba en los arreglos `X_train` y `y_train`:"
      ],
      "metadata": {
        "id": "FvGY61ii36eJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Procesamos datos de prueba \n",
        "dft = df.loc[df.iloc[:,19].isin(['F'])] ## 'F' = test set\n",
        "dft2 = dft['Y']\n",
        "dft1 = dft[['X1','X2','X3','X4','X5','X6','X7','X8','X9','X10']] ## Regresores"
      ],
      "metadata": {
        "id": "3DgWcQZe36eJ"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estandarizamos los datos de los regresores `X_train` y `X_test` restando la media y dividiendo entre la varianza."
      ],
      "metadata": {
        "id": "ZFUp6FdS36eJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.std(numeric_only = True) \n",
        "df1.mean(numeric_only = True)\n",
        "df1 = df1 - df1.mean(numeric_only = True)\n",
        "df1 = df1 / df1.std(numeric_only = True) \n",
        "X_train = df1.to_numpy()   ## Predictors\n",
        "X_train = sm.add_constant(X_train)\n",
        "y_train = df2.to_numpy()   ## Outcome\n",
        "\n",
        "# TRANSFORMATIONS\n",
        "# add squares of raw features\n",
        "tf1 = np.power(x1, 2)\n",
        "tf2 = np.power(x2, 2)\n",
        "tf3 = np.power(x3, 2)\n",
        "X_train = np.column_stack((X_train, tf1, tf2, tf3))\n",
        "# add roots\n",
        "tf4 = np.sqrt(x1)\n",
        "tf5 = np.sqrt(x2)\n",
        "tf6 = np.sqrt(x3)\n",
        "X_train = np.column_stack((X_train, tf4, tf5, tf6))\n",
        "# add logarithms\n",
        "tf7 = np.log(x1)\n",
        "tf8 = np.log(x2)\n",
        "tf9 = np.log(x3)\n",
        "X_train = np.column_stack((X_train, tf4, tf5, tf6))\n",
        "# add products\n",
        "tf10 = np.multiply(x1, x2)\n",
        "tf11 = np.multiply(x2, x3)\n",
        "tf12 = np.multiply(x1, x3)\n",
        "X_train = np.column_stack((X_train, tf10, tf11, tf12))"
      ],
      "metadata": {
        "id": "BqsrHU-536eJ",
        "outputId": "4967f721-ce2b-4fe8-9b91-78232bec5589",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-932b482700cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtf3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m# add roots\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mtf4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mcolumn_stack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36mcolumn_stack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 288 and the array at index 1 has size 50"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dft1.std(numeric_only = True) \n",
        "dft1.mean(numeric_only = True)\n",
        "dft1 = dft1 - dft1.mean(numeric_only = True)\n",
        "dft1 = dft1 / dft1.std(numeric_only = True) \n",
        "X_test = dft1.to_numpy()   ## Predictors\n",
        "X_test = sm.add_constant(X_test)\n",
        "y_test = dft2.to_numpy()   ## Outcome\n",
        "\n",
        "# TRANSFORMATIONS\n",
        "# add squares of raw features\n",
        "tf1 = np.power(x1, 2)\n",
        "tf2 = np.power(x2, 2)\n",
        "tf3 = np.power(x3, 2)\n",
        "X = np.column_stack((X, tf1, tf2, tf3))\n",
        "# add roots\n",
        "tf4 = np.sqrt(x1)\n",
        "tf5 = np.sqrt(x2)\n",
        "tf6 = np.sqrt(x3)\n",
        "X = np.column_stack((X, tf4, tf5, tf6))\n",
        "# add logarithms\n",
        "tf7 = np.log(x1)\n",
        "tf8 = np.log(x2)\n",
        "tf9 = np.log(x3)\n",
        "X = np.column_stack((X, tf4, tf5, tf6))\n",
        "# add products\n",
        "tf10 = np.multiply(x1, x2)\n",
        "tf11 = np.multiply(x2, x3)\n",
        "tf12 = np.multiply(x1, x3)\n",
        "X = np.column_stack((X, tf10, tf11, tf12))"
      ],
      "metadata": {
        "id": "FmH8rdovM4O9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación, obtenemos un modelo de predicción de los datos de entrenamiento usando regresión lineal."
      ],
      "metadata": {
        "id": "Tj0Pzrbf36eJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model   = sm.OLS(y_train, X_train)\n",
        "results = model.fit()\n",
        "#print(results.summary()) \n",
        "#print(results.params)"
      ],
      "metadata": {
        "id": "8ON7WxxR36eK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, calculamos los errores entre la predicción `y_pred` y los datos de entrenamiento `y_train`. Los errores son representados por un histograma."
      ],
      "metadata": {
        "id": "Gusdya9P36eK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred     = pronostica(results.params,X_train)\n",
        "error      = y_train - y_pred\n",
        "err_train  = mean_absolute_error(y_pred,y_train)\n",
        "bias_train = bias.bias(y_pred,y_train, axis=0)\n",
        "df         = pd.DataFrame(error,y_train)"
      ],
      "metadata": {
        "id": "FU9kgMvU36eL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dibuja_hist(df,colour='#777bd4',name='hist11.png',Xlabel=\"Error\",Ylabel=\"Frecuencia\",title=\"Errores de predicción de demanda eléctrica (entrenamiento)\")"
      ],
      "metadata": {
        "id": "0sKG4Kui36eL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, utilizamos el modelo obtenido con los datos de entrenamiento para predecir los datos de prueba. Además,  calculamos los errores entre la predicción `y_pred2` y los datos de prueba $Yt$. Los errores de la predicción con datos de prueba son representados por un histograma."
      ],
      "metadata": {
        "id": "fmGnl57J36eM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred2   = pronostica(results.params,X_test) #print(y_pred2)\n",
        "error2    = y_test - y_pred2\n",
        "err_test  = mean_absolute_error(y_test,y_pred2)\n",
        "bias_test = bias.bias(y_test,y_pred2,axis=0)\n",
        "df        = pd.DataFrame(error2,y_test)"
      ],
      "metadata": {
        "id": "FC1aHj0f36eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dibuja_hist(df,colour='#76ced6',name='hist12.png',Xlabel=\"Error\",Ylabel=\"Frecuencia\",title=\"Errores de predicción de demanda eléctrica (prueba)\")"
      ],
      "metadata": {
        "id": "ps1Mzex236eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, comparamos el **error absoluto medio (MAE)** y **bias** de los datos de entrenamiento así como de los datos de prueba en la predicción de demanda eléctrica."
      ],
      "metadata": {
        "id": "Htn6tkPb36eM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MAE y bias del modelo de regresión con datos de entrenamiento:\", err_train, \",\" , bias_train)\n",
        "print(\"MAE y bias del modelo de regresión con datos de prueba:\" , err_test, \",\" , bias_test) "
      ],
      "metadata": {
        "id": "MjX6GBAq36eM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}